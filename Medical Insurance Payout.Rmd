---
title: "Medical Insurance Payout"
output:
  word_document: default
  html_document:
    df_print: paged
---

OBJECTIVE: ACME Insurance Inc. offers affordable health insurance to thousands of customer all over the United States. Our task is to create an automated system to estimate the annual medical expenditure for new customers, using information such as their age, sex, BMI, children, smoking habits and region of residence.

Estimates from this system will be used to determine the annual insurance premium (amount paid every month) offered to the customer.

```{r}
library(ggplot2)
library(gridExtra)
library(corrplot)
```

Load the data

```{r}
expenses <- read.csv("/Users/srushtigupte/Desktop/R/expenses.csv", header = TRUE)
head(expenses)
```
There are seven columns and all oof them are explained below.
Age: Insurance contractor’s age
Sex: Insurance contractor’s gender, [female, male]
BMI: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight(kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
Children: Number of children covered by health insurance / Number of dependents
Smoker: Smoking, [yes, no]
Region: The beneficiary's residential area in Bangladesh [northeast,southeast, southwest, northwest]
Charges: Individual medical costs billed by health insurance

```{r}
```

Adding a column of risk of higher insurance payout to indicate the charges were higher than $10,000 or not.
```{r}
expenses["risk"] <- 0
expenses$risk <- ifelse(expenses$charges > 10000, "Yes", "No")
head(expenses)
```
EDA

Distribution of all the variables

```{r}
# Create individual ggplot plots
hist(expenses$age)
ggplot(expenses, aes(x = sex)) + geom_bar()
hist(expenses$bmi)
hist(expenses$children)
ggplot(expenses, aes(x = smoker)) + geom_bar()
ggplot(expenses, aes(x = region)) + geom_bar()
ggplot(expenses, aes(x = charges)) + geom_histogram()
ggplot(expenses, aes(x = risk)) + geom_bar()
```
Let's create a plot to understand relation between all the numeric variables.

```{r}
num_col <- unlist(lapply(expenses, is.numeric))
plot(expenses[,num_col])
```
In this plot we can see some of the specific trends that can be useful later in the projects.
1. Interestingly, when the number of children is greater than 3 then charges is significantly low.
2. We can see three bands formation on the charges/age plot

```{r}
ggplot(expenses, aes(x = children, y = charges)) +
  geom_point(aes(col = region))
ggplot(expenses, aes(x = age, y = charges)) +
  geom_point(aes(col = smoker))
```
In the first graph of children/charges there is no significant behavior. However, we can clearly see the three bands formation can be distinguished by smokers and non-smokers, smokers having higher charges and non-smokes having lower charges.

Correlation between continuous variables
```{r}
corrplot.mixed(round(cor(expenses[,num_col]), 2), lower.col = "black")
```
Correlation between the charges and the other variables is not even greater than 50%, but still age is the factor with highest correlation of 0.30. 

Distribution of Categorical Variables

```{r}
str(expenses)

```
```{r}
expenses$sex <- as.factor(expenses$sex)
expenses$smoker <- as.factor(expenses$smoker)
expenses$region <- as.factor(expenses$region)
str(expenses)
```
```{r}
sex_boxplot <- ggplot(expenses, aes(x = sex, y = charges)) +
  geom_boxplot()
smoker_boxplot <- ggplot(expenses, aes(x = smoker, y = charges)) +
  geom_boxplot()
region_boxplot <- ggplot(expenses, aes(x = region, y = charges)) +
  geom_boxplot()
grid.arrange(sex_boxplot, smoker_boxplot, region_boxplot, ncol = 3)

```

Preparing dataset for Modelling

We will divide the dataset into 20-80 ratio. 20% for testing and 80% for training.

```{r}
train_n <- round(0.8*nrow(expenses))
train_indices <- sample(1:nrow(expenses), train_n)
train_expenses <- expenses[train_indices, ]
test_expenses <- expenses[-train_indices, ]
```
Now will create a formula for our model to compare all the variable with charges

```{r}
formula_1 <- as.formula("charges ~ age + sex + bmi + children + smoker + region")
```
Building first Linear Regression Model

```{r}
model_1 <- lm(formula_1, data = train_expenses)
summary(model_1)
```
From the output above, following are the abservations that can be made:
Residuals:

The residuals represent the differences between the observed values and the values predicted by the model. The summary statistics for the residuals indicate that they range from -10,610 to 30,205, with quartiles at -3,057, -1,034, and 1,657, respectively.
Coefficients:

Intercept: The intercept term (-12,584.74) represents the estimated value of the response variable when all predictor variables are zero.
age: For each additional year of age, the estimated response variable increases by approximately $258.88.
sexmale: The coefficient for the 'sex' variable indicates that being male is associated with a decrease in the estimated response variable by $115.15, but this effect is not statistically significant (p-value > 0.05).
bmi: For each unit increase in BMI, the estimated response variable increases by $362.36.
children: For each additional child, the estimated response variable increases by $430.89, and this effect is statistically significant at the 5% level (p-value < 0.05).
smokeryes: Being a smoker is associated with a significant increase in the estimated response variable by $23,393.56.
regionnorthwest, regionsoutheast, regionsouthwest: These coefficients represent the differences in the estimated response variable between the respective regions and the reference region (presumably northeast). However, only the coefficient for 'regionsouthwest' is statistically significant at the 5% level (p-value < 0.05).
Residual standard error:

The residual standard error is approximately 6031, indicating the typical deviation of the observed values from the predicted values by the model.
Multiple R-squared and Adjusted R-squared:

The multiple R-squared value of 0.7529 suggests that approximately 75.29% of the variance in the response variable is explained by the predictors included in the model. The adjusted R-squared value of 0.751 is similar, indicating that the model's explanatory power remains high even after adjusting for the number of predictors.
F-statistic:

The F-statistic tests the overall significance of the model. In this case, the F-statistic is 404 with a very low p-value (< 2.2e-16), indicating that the model as a whole is highly significant.

Overall, this model suggests that age, BMI, number of children, smoking status, and region (specifically southwest) are significant predictors of the response variable, while gender does not appear to be a significant predictor in this model. The model as a whole is highly significant in explaining the variance in the response variable.

For improving this model, we will eliminate the less significant variables here (sex)

```{r}
formula_2 <- as.formula("charges ~ age + bmi + children + smoker + region")
model_2 <- lm(formula_2, data = train_expenses)
summary(model_2)
```
Comparing both the Models

Adjusted R-squared: Both models have very similar adjusted R-squared values, indicating that they explain a similar proportion of variance in the response variable.

F-statistic: Model 2 has a higher F-statistic (462.1) compared to Model 1 (404), suggesting that Model 2 is better at explaining the variability in the response variable compared to Model 1.

Residual standard error: Both models have similar residual standard errors, suggesting that they have similar levels of variation in their residuals.

Number of predictors: Model 1 includes an additional predictor ('sex') compared to Model 2.

Based on these comparisons, Model 2 appears to be slightly better than Model 1. It has a higher F-statistic, indicating a better overall fit to the data. Additionally, both models have similar adjusted R-squared values and residual standard errors, suggesting that Model 2 achieves comparable performance with fewer predictors, which is generally preferable as it may reduce complexity and overfitting.

Therefore, Model 2 is considered better because it achieves similar or slightly better performance with fewer predictors, which can lead to a more parsimonious and interpretable model.

```{r}
test_expenses$prediction <- predict(model_2, newdata = test_expenses)
ggplot(test_expenses, aes(x = prediction, y = charges)) +
  geom_point() + 
  geom_abline() + 
  ggtitle("Prediction vs Real Charges")
```

```{r}
test_expenses$residual <- test_expenses$charges - test_expenses$prediction

ggplot(test_expenses, aes(x = prediction, y = residual)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = 3) +
  ggtitle("Residuals vs. Linear Model Prediction")
```
```{r}
ggplot(test_expenses, aes(residual)) +
  geom_histogram() +
  ggtitle("Histogram of Residuals")
```
Error in all the above graphs is close to zero, that means the linear model is giving out pretty accurate charges of healthcare based on the factors.

Now we will test the model with some dummy observations.

```{r}
dummy_data <- data.frame(age = 29,
                         bmi = 27,
                         children = 0,
                         smoker = "no",
                         region = "northeast")
dummy_data$predicted_charges <- round(predict(model_2, dummy_data), 2)
```
We have successfully created a data frame with the new dummy data and our predicting model to predict the charges.

```{r}
new_row <- data.frame(age = 30,
                      bmi = 25,
                      children = 2,
                      smoker = "yes",
                      region = "southeast")
new_row$predicted_charges <- round(predict(model_2, new_row), 2)
dummy_data <- rbind(dummy_data, new_row)

head(dummy_data)
```

